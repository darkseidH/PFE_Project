{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d5fe29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#!conda install -c conda-forge gdal pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd88c0e",
   "metadata": {},
   "source": [
    "# Data Processing and Feature Engineering Pipeline\n",
    "\n",
    "**Objective:** This notebook details the steps to preprocess raw data, handle missing values, perform feature engineering, and prepare a cleaned dataset for further analysis or modeling.\n",
    "\n",
    "**Sections:**\n",
    "1.  Initial Data Loading and Type Inspection\n",
    "2.  Date Column Processing and Feature Creation\n",
    "3.  Handling Missing Values in Date Columns\n",
    "4.  Processing Boolean Flag Columns (FL_)\n",
    "5.  Processing Categorical Code Columns (CD_)\n",
    "6.  Processing General Text Columns (GN_)\n",
    "7.  Processing Numerical Columns (NM_)\n",
    "8.  Feature Engineering from Existing Data\n",
    "9.  Final Data Cleaning and Validation\n",
    "10. Saving Processed Data (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_1_intro",
   "metadata": {},
   "source": [
    "## 1. Initial Data Loading and Type Inspection\n",
    "This section loads the raw dataset and performs an initial inspection of data types. The original data types are also saved to a CSV for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62a7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data types summary:\n",
      "object     133\n",
      "int64       31\n",
      "float64     14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original data types saved to './data/original_data_types.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the raw dataset\n",
    "# low_memory=False is used to ensure correct type inference for mixed-type columns\n",
    "df = pd.read_csv(\"data/raw_data.csv\", low_memory=False)\n",
    "\n",
    "# Display initial data type distribution\n",
    "print(\"Initial data types summary:\")\n",
    "print(df.dtypes.value_counts())  #\n",
    "\n",
    "# Save the original data types to a CSV file for documentation purposes\n",
    "df.dtypes.reset_index().rename(columns={\"index\": \"Column Name\", 0: \"Data Type\"}).to_csv(\n",
    "    \"./data/original_data_types.csv\", index=False\n",
    ")  #\n",
    "print(\"\\nOriginal data types saved to './data/original_data_types.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_2_intro",
   "metadata": {},
   "source": [
    "## 2. Date Column Processing and Feature Creation\n",
    "This section focuses on columns representing dates. It involves:\n",
    "* Converting relevant columns to datetime objects.\n",
    "* Filtering the DataFrame based on non-missing key date fields.\n",
    "* Calculating delivery delay as a preliminary feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefe1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 61 columns to datetime objects.\n"
     ]
    }
   ],
   "source": [
    "# Identify columns that represent dates (conventionally prefixed with \"DT_\")\n",
    "date_cols = df.columns[df.columns.str.startswith(\"DT_\")]  #\n",
    "\n",
    "# Convert these columns to datetime objects\n",
    "# 'errors=\"coerce\"' will turn unparseable dates into NaT (Not a Time)\n",
    "df[date_cols] = df[date_cols].apply(pd.to_datetime, errors=\"coerce\")  #\n",
    "print(f\"Converted {len(date_cols)} columns to datetime objects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b858e42-31af-4ce9-a3b9-24f7a40d7747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame after filtering for essential dates: (10027, 178)\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to include only rows where key delivery dates are present\n",
    "# This is crucial for calculating delays and other time-based features accurately.\n",
    "# Using .copy() to avoid SettingWithCopyWarning and ensure 'filtered_df' is a new DataFrame.\n",
    "filtered_df = df[\n",
    "    df[\"DT_EXPECTED_DELIVERY_TO_FINAL_CUSTOMER_DATE\"].notna()\n",
    "    & df[\"DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE\"].notna()\n",
    "].copy()  #\n",
    "\n",
    "print(f\"Shape of DataFrame after filtering for essential dates: {filtered_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2578746c-8a23-4654-9409-0e673e1b1761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated 'Delay_days'.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the delivery delay in days\n",
    "# This is the difference between actual arrival and expected delivery.\n",
    "filtered_df[\"Delay_days\"] = (\n",
    "    filtered_df[\"DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE\"]\n",
    "    - filtered_df[\"DT_EXPECTED_DELIVERY_TO_FINAL_CUSTOMER_DATE\"]\n",
    ")  #\n",
    "print(\"Calculated 'Delay_days'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea61c83-467b-4010-83d7-056f773e4dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shape of filtered_df: (10027, 179)\n"
     ]
    }
   ],
   "source": [
    "# Verify the shape of the filtered DataFrame\n",
    "print(f\"Current shape of filtered_df: {filtered_df.shape}\")  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e75744e2-610b-448a-9c5e-fcf250be47b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of orders delayed: 4819\n",
      "Percentage of orders delayed: 48.06%\n"
     ]
    }
   ],
   "source": [
    "# Analyze the calculated delays\n",
    "# A positive delay means the order arrived after the expected date.\n",
    "order_delayed_boolean_series = filtered_df[\"Delay_days\"].dt.days > 0  #\n",
    "\n",
    "# Count the number of orders that were delayed\n",
    "num_order_delayed = order_delayed_boolean_series.sum()  #\n",
    "\n",
    "# Calculate the percentage of delayed orders\n",
    "percentage_delayed = (\n",
    "    (num_order_delayed / len(filtered_df)) * 100 if len(filtered_df) > 0 else 0\n",
    ")  #\n",
    "\n",
    "print(f\"Number of orders delayed: {num_order_delayed}\")\n",
    "print(f\"Percentage of orders delayed: {percentage_delayed:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_2_date_col_selection",
   "metadata": {},
   "source": [
    "### 2.1 Selecting and Dropping Date Columns\n",
    "Keep only essential date columns for further processing and feature engineering. Other date columns are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb235bb9-8067-474e-8b43-248957cb84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of date columns to retain for analysis\n",
    "columns_to_keep = [\n",
    "    \"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\",\n",
    "    \"DT_VEHICLE_FACTORY_PRODUCTION_DATE\",\n",
    "    \"DT_VEHICLE_PASSED_TO_SALES_DATE\",\n",
    "    \"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\",\n",
    "    \"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\",\n",
    "    \"DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE\",\n",
    "    \"DT_CUSTOMS_OFFICE_INBOUND_DATE\",\n",
    "    \"DT_CUSTOMS_OFFICE_OUTBOUND_DATE\",\n",
    "    \"DT_EXPECTED_DELIVERY_TO_FINAL_CUSTOMER_DATE\",\n",
    "]  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38bb0154-5abf-4d58-aee9-c8735d8457e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 52 non-essential date columns.\n",
      "Remaining date columns: ['DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE', 'DT_VEHICLE_FACTORY_PRODUCTION_DATE', 'DT_VEHICLE_PASSED_TO_SALES_DATE', 'DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE', 'DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE', 'DT_CUSTOMS_OFFICE_INBOUND_DATE', 'DT_CUSTOMS_OFFICE_OUTBOUND_DATE', 'DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE', 'DT_EXPECTED_DELIVERY_TO_FINAL_CUSTOMER_DATE']\n"
     ]
    }
   ],
   "source": [
    "# Re-identify all date columns currently in filtered_df (prefixed with \"DT_\")\n",
    "current_date_cols = filtered_df.columns[filtered_df.columns.str.startswith(\"DT_\")]\n",
    "\n",
    "# Determine which date columns to drop\n",
    "date_cols_to_drop = [col for col in current_date_cols if col not in columns_to_keep]\n",
    "\n",
    "if date_cols_to_drop:\n",
    "    filtered_df.drop(columns=date_cols_to_drop, inplace=True)  #\n",
    "    print(f\"Dropped {len(date_cols_to_drop)} non-essential date columns.\")\n",
    "else:\n",
    "    print(\"No date columns to drop based on the keep list.\")\n",
    "\n",
    "# Update the list of active date columns after dropping\n",
    "date_cols = filtered_df.columns[filtered_df.columns.str.startswith(\"DT_\")]  #\n",
    "print(f\"Remaining date columns: {date_cols.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4879ca3-493a-4faf-ba00-66d4e7d0f21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in remaining date columns after initial filtering and selection:\n",
      "DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE                    52\n",
      "DT_VEHICLE_FACTORY_PRODUCTION_DATE                    7877\n",
      "DT_VEHICLE_PASSED_TO_SALES_DATE                         52\n",
      "DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE     307\n",
      "DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE       5257\n",
      "DT_CUSTOMS_OFFICE_INBOUND_DATE                        7593\n",
      "DT_CUSTOMS_OFFICE_OUTBOUND_DATE                       9190\n",
      "DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE            0\n",
      "DT_EXPECTED_DELIVERY_TO_FINAL_CUSTOMER_DATE              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the remaining date columns\n",
    "print(\n",
    "    \"\\nMissing values in remaining date columns after initial filtering and selection:\"\n",
    ")\n",
    "print(filtered_df[date_cols].isna().sum())  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_3_intro",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values in Date Columns\n",
    "This section addresses missing (NaT) values in critical date columns. Strategies include:\n",
    "* Dropping rows where key chronological dates are missing.\n",
    "* Imputing other missing dates based on median time differences from related, non-missing dates.\n",
    "* Creating flag columns to indicate where imputation occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc47e703-c5d3-45c9-be72-59f185703de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 52 rows due to missing key order/sales dates.\n",
      "Shape of DataFrame after dropping rows with missing essential dates: (9975, 127)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows if 'DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE' or 'DT_VEHICLE_PASSED_TO_SALES_DATE' are missing,\n",
    "# as these are fundamental for many downstream calculations and imputations.\n",
    "initial_rows = len(filtered_df)\n",
    "filtered_df.dropna(\n",
    "    subset=[\"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\", \"DT_VEHICLE_PASSED_TO_SALES_DATE\"],\n",
    "    inplace=True,\n",
    ")  #\n",
    "rows_dropped = initial_rows - len(filtered_df)\n",
    "print(f\"Dropped {rows_dropped} rows due to missing key order/sales dates.\")\n",
    "print(\n",
    "    f\"Shape of DataFrame after dropping rows with missing essential dates: {filtered_df.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impute_production_date",
   "metadata": {},
   "source": [
    "### 3.1 Imputing `DT_VEHICLE_FACTORY_PRODUCTION_DATE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca94444-2d2e-470d-9e1d-1a94f4d00cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median production time: 81.0 days.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the median production time (days from order to production) for non-missing rows\n",
    "production_time_series = (\n",
    "    filtered_df[\"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"].dropna()\n",
    "    - filtered_df.loc[\n",
    "        filtered_df[\"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"].notna(),\n",
    "        \"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\",\n",
    "    ]\n",
    ").dt.days  #\n",
    "\n",
    "median_production_time_days = production_time_series.median()  #\n",
    "print(f\"Median production time: {median_production_time_days} days.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f70c885-0dc3-4ade-8b8d-df35d764aeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7825 production dates will be imputed.\n"
     ]
    }
   ],
   "source": [
    "# Create a flag column to indicate if the production date was originally missing (and thus imputed)\n",
    "filtered_df[\"MISSING_PRODUCTION_DATE_FLAG\"] = (\n",
    "    filtered_df[\"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"].isna().astype(int)\n",
    ")  #\n",
    "print(\n",
    "    f\"{filtered_df['MISSING_PRODUCTION_DATE_FLAG'].sum()} production dates will be imputed.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6988532a-a23a-4efa-906d-9a6af1d2c4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed missing 'DT_VEHICLE_FACTORY_PRODUCTION_DATE' values.\n"
     ]
    }
   ],
   "source": [
    "# Impute missing production dates by adding the median production time to the order entry date\n",
    "if pd.notna(median_production_time_days):\n",
    "    imputation_values = filtered_df[\n",
    "        \"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\"\n",
    "    ] + pd.Timedelta(days=median_production_time_days)\n",
    "    filtered_df[\"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"] = filtered_df[\n",
    "        \"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"\n",
    "    ].fillna(imputation_values)  #\n",
    "    print(\"Imputed missing 'DT_VEHICLE_FACTORY_PRODUCTION_DATE' values.\")\n",
    "else:\n",
    "    print(\n",
    "        \"Median production time is NaN, skipping imputation for 'DT_VEHICLE_FACTORY_PRODUCTION_DATE'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58fd825b-996d-492b-b192-b3decb7a0a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where imputed production date is before order date: 0\n"
     ]
    }
   ],
   "source": [
    "# Validate imputation: Check if any imputed production dates are earlier than their order dates\n",
    "invalid_imputed_rows = filtered_df[\n",
    "    (filtered_df[\"MISSING_PRODUCTION_DATE_FLAG\"] == 1)\n",
    "    & (\n",
    "        filtered_df[\"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"]\n",
    "        < filtered_df[\"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\"]\n",
    "    )\n",
    "]  #\n",
    "print(\n",
    "    f\"Number of rows where imputed production date is before order date: {len(invalid_imputed_rows)}\"\n",
    ")\n",
    "# Further investigation or adjustment might be needed if this count is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4baea3cf-a90a-4d21-ab6e-f75438b55068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'DT_VEHICLE_FACTORY_PRODUCTION_DATE' after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify that all NaNs in 'DT_VEHICLE_FACTORY_PRODUCTION_DATE' have been handled\n",
    "print(\n",
    "    f\"Missing values in 'DT_VEHICLE_FACTORY_PRODUCTION_DATE' after imputation: {filtered_df['DT_VEHICLE_FACTORY_PRODUCTION_DATE'].isna().sum()}\"\n",
    ")  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impute_ready_to_ship_date",
   "metadata": {},
   "source": [
    "### 3.2 Imputing `DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f3d44c1-9b57-4e63-8fca-e21e1e54602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median lag from production to ready-to-ship: -12.0 days.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the median lag time from production to 'ready to ship'\n",
    "ready_to_ship_lag_series = (\n",
    "    filtered_df[\"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\"].dropna()\n",
    "    - filtered_df.loc[\n",
    "        filtered_df[\"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\"].notna(),\n",
    "        \"DT_VEHICLE_FACTORY_PRODUCTION_DATE\",\n",
    "    ]\n",
    ").dt.days  #\n",
    "\n",
    "median_ready_to_ship_lag_days = ready_to_ship_lag_series.median()  #\n",
    "print(\n",
    "    f\"Median lag from production to ready-to-ship: {median_ready_to_ship_lag_days} days.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7be00102-aa3b-4d47-8989-9123bdb7092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 'ready to ship' dates will be imputed.\n"
     ]
    }
   ],
   "source": [
    "# Create a flag for missing 'ready to ship' dates\n",
    "filtered_df[\"MISSING_READY_TO_SHIP_FLAG\"] = (\n",
    "    filtered_df[\"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\"].isna().astype(int)\n",
    ")  #\n",
    "print(\n",
    "    f\"{filtered_df['MISSING_READY_TO_SHIP_FLAG'].sum()} 'ready to ship' dates will be imputed.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdd85ff1-ba42-4648-9a7d-0ea8cb551050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed missing 'DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE' values.\n"
     ]
    }
   ],
   "source": [
    "# Impute missing 'ready to ship' dates using the median lag from the (now imputed) production date\n",
    "if pd.notna(median_ready_to_ship_lag_days):\n",
    "    imputation_values_rts = filtered_df[\n",
    "        \"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"\n",
    "    ] + pd.Timedelta(days=median_ready_to_ship_lag_days)\n",
    "    filtered_df[\"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\"] = filtered_df[\n",
    "        \"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\"\n",
    "    ].fillna(imputation_values_rts)  #\n",
    "    print(\n",
    "        \"Imputed missing 'DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE' values.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Median ready-to-ship lag is NaN, skipping imputation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0270f41-4bcf-48b3-b9f9-8e85906eb54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE' after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify imputation\n",
    "print(\n",
    "    f\"Missing values in 'DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE' after imputation: {filtered_df['DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE'].isna().sum()}\"\n",
    ")  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impute_shipping_order_date",
   "metadata": {},
   "source": [
    "### 3.3 Imputing `DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1413a8b1-16c5-4dbd-83c7-0937211c4566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th percentile lag from ready-to-ship to shipping order creation: 7.0 days.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 90th percentile lag from 'ready to ship' to 'shipping order creation'\n",
    "# Using a higher percentile (e.g., 90th) can be a strategy for more conservative estimation if quick turnarounds are outliers.\n",
    "shipping_order_lag_series = (\n",
    "    filtered_df[\"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\"].dropna()\n",
    "    - filtered_df.loc[\n",
    "        filtered_df[\"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\"].notna(),\n",
    "        \"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\",\n",
    "    ]\n",
    ").dt.days  #\n",
    "\n",
    "# Calculate the 90th percentile of this lag\n",
    "percentile_90_shipping_order_lag_days = shipping_order_lag_series.quantile(0.9)  #\n",
    "print(\n",
    "    f\"90th percentile lag from ready-to-ship to shipping order creation: {percentile_90_shipping_order_lag_days} days.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a13b80c8-c0a9-45b3-89a6-26884e7b728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5205 'shipping order creation' dates will be imputed.\n"
     ]
    }
   ],
   "source": [
    "# Flag missing shipping order dates\n",
    "filtered_df[\"MISSING_SHIPPING_ORDER_FLAG\"] = (\n",
    "    filtered_df[\"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\"].isna().astype(int)\n",
    ")  #\n",
    "print(\n",
    "    f\"{filtered_df['MISSING_SHIPPING_ORDER_FLAG'].sum()} 'shipping order creation' dates will be imputed.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c048706-5e28-4469-a499-131528c67401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed missing 'DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE' values.\n"
     ]
    }
   ],
   "source": [
    "# Impute missing shipping order dates using the 90th percentile lag\n",
    "if pd.notna(percentile_90_shipping_order_lag_days):\n",
    "    imputation_values_so = filtered_df[\n",
    "        \"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\"\n",
    "    ] + pd.Timedelta(days=percentile_90_shipping_order_lag_days)\n",
    "    filtered_df[\"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\"] = filtered_df[\n",
    "        \"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\"\n",
    "    ].fillna(imputation_values_so)  #\n",
    "    print(\"Imputed missing 'DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE' values.\")\n",
    "else:\n",
    "    print(\"90th percentile shipping order lag is NaN, skipping imputation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2578746c-8a23-4654-9409-0e673e1b1761_validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where imputed shipping order date is before ready-to-ship date: 505\n"
     ]
    }
   ],
   "source": [
    "# Validate imputation: Check for rows where shipping order date is before ready-to-ship date\n",
    "# This could happen if the lag used for imputation is negative or too small, or due to original data issues.\n",
    "invalid_shipping_order_rows = filtered_df[\n",
    "    filtered_df[\"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\"]\n",
    "    < filtered_df[\"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\"]\n",
    "]  #\n",
    "print(\n",
    "    f\"Number of rows where imputed shipping order date is before ready-to-ship date: {len(invalid_shipping_order_rows)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "112a97b1-e520-4d8d-bca2-5447d20456c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag these chronologically invalid rows\n",
    "filtered_df[\"INVALID_SHIPPING_ORDER_TIMING_FLAG\"] = (\n",
    "    filtered_df[\"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\"]\n",
    "    < filtered_df[\"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\"]\n",
    ").astype(int)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceb186cf-e8ba-4b9a-b0a2-5eeaa82ca9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with invalid shipping order timing: 505\n",
      "Percentage of dataset: 5.06%\n"
     ]
    }
   ],
   "source": [
    "# Analyze the extent of invalid rows\n",
    "invalid_timing_count = filtered_df[\"INVALID_SHIPPING_ORDER_TIMING_FLAG\"].sum()  #\n",
    "percentage_invalid_timing = (\n",
    "    (invalid_timing_count / len(filtered_df)) * 100 if len(filtered_df) > 0 else 0\n",
    ")\n",
    "print(f\"Number of rows with invalid shipping order timing: {invalid_timing_count}\")\n",
    "print(f\"Percentage of dataset: {percentage_invalid_timing:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "148fc5c1-80c2-4da3-9e44-40a9dacca1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 505 rows due to invalid shipping order timing.\n",
      "Shape of DataFrame after handling invalid shipping order timing: (9470, 130)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with invalid shipping order timing as they represent a data inconsistency\n",
    "initial_rows_before_timing_drop = len(filtered_df)\n",
    "filtered_df = filtered_df[\n",
    "    filtered_df[\"INVALID_SHIPPING_ORDER_TIMING_FLAG\"] == 0\n",
    "].copy()  #\n",
    "rows_dropped_timing = initial_rows_before_timing_drop - len(filtered_df)\n",
    "print(f\"Dropped {rows_dropped_timing} rows due to invalid shipping order timing.\")\n",
    "\n",
    "# Drop the flag column as it's no longer needed after filtering\n",
    "filtered_df.drop(columns=[\"INVALID_SHIPPING_ORDER_TIMING_FLAG\"], inplace=True)  #\n",
    "print(\n",
    "    f\"Shape of DataFrame after handling invalid shipping order timing: {filtered_df.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0c7d6c0-3929-4293-85bd-4ae4df9c4c64_updated",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of date columns after imputation and cleaning steps:\n",
      "    DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE DT_VEHICLE_FACTORY_PRODUCTION_DATE  \\\n",
      "210                           2024-03-29                         2024-06-18   \n",
      "693                           2023-03-31                         2023-06-20   \n",
      "734                           2023-06-06                         2023-08-26   \n",
      "750                           2022-06-02                         2022-08-22   \n",
      "801                           2024-09-10                         2024-11-30   \n",
      "\n",
      "    DT_VEHICLE_PASSED_TO_SALES_DATE  \\\n",
      "210                      2024-05-21   \n",
      "693                      2023-04-13   \n",
      "734                      2023-10-04   \n",
      "750                      2022-07-05   \n",
      "801                      2024-10-10   \n",
      "\n",
      "    DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE  \\\n",
      "210                                         2024-05-21   \n",
      "693                                         2023-04-13   \n",
      "734                                         2023-10-04   \n",
      "750                                         2022-07-06   \n",
      "801                                         2024-10-10   \n",
      "\n",
      "    DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE  \\\n",
      "210                                      2024-05-28   \n",
      "693                                      2023-04-20   \n",
      "734                                      2023-10-11   \n",
      "750                                      2022-07-13   \n",
      "801                                      2024-10-17   \n",
      "\n",
      "    DT_CUSTOMS_OFFICE_INBOUND_DATE DT_CUSTOMS_OFFICE_OUTBOUND_DATE  \\\n",
      "210                            NaT                             NaT   \n",
      "693                            NaT                             NaT   \n",
      "734                     2023-10-25                             NaT   \n",
      "750                     2022-07-27                      2022-07-29   \n",
      "801                     2024-12-04                      2024-12-05   \n",
      "\n",
      "    DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE  \\\n",
      "210                                    2024-07-11   \n",
      "693                                    2023-06-10   \n",
      "734                                    2023-11-24   \n",
      "750                                    2022-08-22   \n",
      "801                                    2024-12-24   \n",
      "\n",
      "    DT_EXPECTED_DELIVERY_TO_FINAL_CUSTOMER_DATE  \n",
      "210                                  2024-07-07  \n",
      "693                                  2023-05-28  \n",
      "734                                  2023-11-19  \n",
      "750                                  2022-07-31  \n",
      "801                                  2024-12-23  \n",
      "\n",
      "Missing values check for date columns after all date imputations:\n",
      "DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE                     0\n",
      "DT_VEHICLE_FACTORY_PRODUCTION_DATE                       0\n",
      "DT_VEHICLE_PASSED_TO_SALES_DATE                          0\n",
      "DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE       0\n",
      "DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE          0\n",
      "DT_CUSTOMS_OFFICE_INBOUND_DATE                        7109\n",
      "DT_CUSTOMS_OFFICE_OUTBOUND_DATE                       8663\n",
      "DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE            0\n",
      "DT_EXPECTED_DELIVERY_TO_FINAL_CUSTOMER_DATE              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of the processed date columns and their current state\n",
    "print(\"\\nSample of date columns after imputation and cleaning steps:\")\n",
    "if not filtered_df.empty:\n",
    "    print(filtered_df[date_cols].head())  #\n",
    "    print(\"\\nMissing values check for date columns after all date imputations:\")\n",
    "    print(filtered_df[date_cols].isna().sum())\n",
    "else:\n",
    "    print(\"DataFrame is empty after filtering steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impute_customs_dates",
   "metadata": {},
   "source": [
    "### 3.4 Handling Customs Date Columns (`DT_CUSTOMS_OFFICE_INBOUND_DATE` & `DT_CUSTOMS_OFFICE_OUTBOUND_DATE`)\n",
    "Customs dates are only relevant for international shipments. We will:\n",
    "* Create a flag `IS_INTERNATIONAL` based on the presence of `DT_CUSTOMS_OFFICE_INBOUND_DATE`.\n",
    "* Impute `DT_CUSTOMS_OFFICE_OUTBOUND_DATE` based on the average clearance time for international shipments where both dates are present.\n",
    "* Create a status column `International_Status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30118781-9861-48dc-b3ec-ae3143f54023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'IS_INTERNATIONAL' flag created.\n",
      "IS_INTERNATIONAL\n",
      "0    7109\n",
      "1    2361\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create 'IS_INTERNATIONAL' flag: 1 if customs inbound date is present, 0 otherwise.\n",
    "filtered_df[\"IS_INTERNATIONAL\"] = (\n",
    "    filtered_df[\"DT_CUSTOMS_OFFICE_INBOUND_DATE\"].notna().astype(int)\n",
    ")  #\n",
    "print(\"'IS_INTERNATIONAL' flag created.\")\n",
    "print(filtered_df[\"IS_INTERNATIONAL\"].value_counts(dropna=False))  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3141e4a-b892-4ba3-b781-dcf863e60869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average customs clearance time: 7.39 days (used for imputation if outbound date is missing but inbound exists).\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average customs clearance time in days for shipments where both dates are available\n",
    "mask_both_customs_dates_present = (\n",
    "    filtered_df[\"DT_CUSTOMS_OFFICE_INBOUND_DATE\"].notna()\n",
    "    & filtered_df[\"DT_CUSTOMS_OFFICE_OUTBOUND_DATE\"].notna()\n",
    ")  #\n",
    "\n",
    "average_customs_clearance_days = 0  # Default fallback\n",
    "if mask_both_customs_dates_present.any():\n",
    "    customs_clearance_times = (\n",
    "        filtered_df.loc[\n",
    "            mask_both_customs_dates_present, \"DT_CUSTOMS_OFFICE_OUTBOUND_DATE\"\n",
    "        ]\n",
    "        - filtered_df.loc[\n",
    "            mask_both_customs_dates_present, \"DT_CUSTOMS_OFFICE_INBOUND_DATE\"\n",
    "        ]\n",
    "    ).dt.days  #\n",
    "    average_customs_clearance_days = customs_clearance_times.mean()  #\n",
    "    # Ensure average days is not negative, which would be illogical\n",
    "    if average_customs_clearance_days < 0:\n",
    "        average_customs_clearance_days = 0  # Or handle as an error/warning\n",
    "        print(\n",
    "            f\"Warning: Calculated average customs clearance days is negative ({average_customs_clearance_days}). Setting to 0.\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"Average customs clearance time: {average_customs_clearance_days:.2f} days (used for imputation if outbound date is missing but inbound exists).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30b8aafe-289f-4698-91fd-0fda2221f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 1555 missing 'DT_CUSTOMS_OFFICE_OUTBOUND_DATE' values.\n"
     ]
    }
   ],
   "source": [
    "# Impute missing 'DT_CUSTOMS_OFFICE_OUTBOUND_DATE' where 'DT_CUSTOMS_OFFICE_INBOUND_DATE' exists\n",
    "mask_inbound_present_outbound_missing = (\n",
    "    filtered_df[\"DT_CUSTOMS_OFFICE_INBOUND_DATE\"].notna()\n",
    "    & filtered_df[\"DT_CUSTOMS_OFFICE_OUTBOUND_DATE\"].isna()\n",
    ")  #\n",
    "\n",
    "if mask_inbound_present_outbound_missing.any():\n",
    "    imputed_outbound_dates = filtered_df.loc[\n",
    "        mask_inbound_present_outbound_missing, \"DT_CUSTOMS_OFFICE_INBOUND_DATE\"\n",
    "    ] + pd.to_timedelta(average_customs_clearance_days, unit=\"D\")  #\n",
    "    filtered_df.loc[\n",
    "        mask_inbound_present_outbound_missing, \"DT_CUSTOMS_OFFICE_OUTBOUND_DATE\"\n",
    "    ] = imputed_outbound_dates\n",
    "    print(\n",
    "        f\"Imputed {mask_inbound_present_outbound_missing.sum()} missing 'DT_CUSTOMS_OFFICE_OUTBOUND_DATE' values.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"No 'DT_CUSTOMS_OFFICE_OUTBOUND_DATE' values to impute based on existing inbound dates.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06b5d805-1e0d-46de-ade4-b4a3d89f09ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'International_Status' column created:\n",
      "International_Status\n",
      "Domestic / No Customs Data              7109\n",
      "International (Customs Data Present)    2361\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create 'International_Status' column for easier interpretation\n",
    "filtered_df[\"International_Status\"] = \"International (Customs Data Present)\"  #\n",
    "\n",
    "# Label rows as \"Domestic / No Customs Data\" if 'DT_CUSTOMS_OFFICE_INBOUND_DATE' was originally NaN\n",
    "# This uses the IS_INTERNATIONAL flag created *before* imputation of outbound dates\n",
    "filtered_df.loc[filtered_df[\"IS_INTERNATIONAL\"] == 0, \"International_Status\"] = (\n",
    "    \"Domestic / No Customs Data\"  #\n",
    ")\n",
    "print(\"\\n'International_Status' column created:\")\n",
    "print(filtered_df[\"International_Status\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b516a65-dd26-41c1-ab9d-ba962c2f676a_final_date_nan_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in date columns after all imputation and handling:\n",
      "DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE                     0\n",
      "DT_VEHICLE_FACTORY_PRODUCTION_DATE                       0\n",
      "DT_VEHICLE_PASSED_TO_SALES_DATE                          0\n",
      "DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE       0\n",
      "DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE          0\n",
      "DT_CUSTOMS_OFFICE_INBOUND_DATE                        7109\n",
      "DT_CUSTOMS_OFFICE_OUTBOUND_DATE                       7108\n",
      "DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE            0\n",
      "DT_EXPECTED_DELIVERY_TO_FINAL_CUSTOMER_DATE              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Final check of NaNs in date columns after all imputation steps\n",
    "print(\"\\nMissing values in date columns after all imputation and handling:\")\n",
    "if not filtered_df.empty:\n",
    "    print(filtered_df[date_cols].isna().sum())  #\n",
    "    # Remaining NaNs in customs columns are expected for non-international shipments.\n",
    "else:\n",
    "    print(\"DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_4_intro",
   "metadata": {},
   "source": [
    "## 4. Processing Boolean Flag Columns (FL_)\n",
    "This section handles columns prefixed with `FL_`, which are typically boolean flags (0 or 1).\n",
    "Steps include:\n",
    "* Identifying `FL_` columns.\n",
    "* Dropping specified unnecessary flag columns.\n",
    "* Filling missing values (NaNs) in `FL_IS_DIRECT_SALES_FLAG` with its mode.\n",
    "* Converting all remaining `FL_` columns to integer type (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "febac490-848a-4c3d-91f5-9b4449d17882_inspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'FL_IS_VEHICLE_IN_MARKET_COMPOUND_INVOICEABLE_FLAG' (original df):\n",
      "[nan  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Inspect unique values of a sample FL_ column to understand its structure (e.g., NaN, 0.0, 1.0)\n",
    "if \"FL_IS_VEHICLE_IN_MARKET_COMPOUND_INVOICEABLE_FLAG\" in df.columns:\n",
    "    print(\n",
    "        \"Unique values in 'FL_IS_VEHICLE_IN_MARKET_COMPOUND_INVOICEABLE_FLAG' (original df):\"\n",
    "    )\n",
    "    print(df[\"FL_IS_VEHICLE_IN_MARKET_COMPOUND_INVOICEABLE_FLAG\"].unique())  #\n",
    "else:\n",
    "    print(\n",
    "        \"'FL_IS_VEHICLE_IN_MARKET_COMPOUND_INVOICEABLE_FLAG' not found in original df.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5441d98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 columns starting with 'FL_': ['FL_IS_FIRST_ORDER_INGESTION_FLAG', 'FL_IS_ORDER_ACTIVE_FLAG', 'FL_IS_NEW_VEHICLE_ORDER_DAMAGED_FLAG', 'FL_IS_NEW_VEHICLE_ORDER_IN_NETWORK_FLAG', 'FL_IS_NEW_VEHICLE_ORDER_IN_TRANSPORT_TO_DEALER_FLAG', 'FL_IS_NEW_VEHICLE_ORDER_PRODUCED_FLAG', 'FL_IS_NEW_VEHICLE_ORDER_STORED_FLAG', 'FL_IS_OWNED_NETWORK_SUBJECT_FLAG', 'FL_IS_FACTORY_ORDER_CODE_AVAILABLE_FLAG', 'FL_IS_FINAL_CUSTOMER_ORDER_ISSUED_FLAG', 'FL_IS_ONLINE_SALES_FLAG', 'FL_IS_DIRECT_SALES_FLAG', 'FL_IS_SALES_INVOICE_ISSUED_FLAG', 'FL_IS_PROPERTY_STOCK_FLAG', 'FL_IS_PROPERTY_STOCK_IN_TRANSIT_FLAG', 'FL_IS_PROPERTY_STOCK_ARRIVED_TO_SALES_MARKET_FLAG', 'FL_IS_NETWORK_STOCK_FLAG', 'FL_IS_NETWORK_STOCK_IN_TRANSIT_FLAG', 'FL_IS_VEHICLE_NOT_SOLD_WITH_SALES_INVOICE_FLAG', 'FL_IS_FACTORY_PRODUCTION_PLANNED_FLAG', 'FL_IS_VEHICLE_PASSED_TO_SALES_FLAG', 'FL_IS_VEHICLE_ORDER_IN_PLANT_FLAG', 'FL_IS_SHIPPED_TO_COMPOUND_FLAG', 'FL_IS_VEHICLE_ORDER_IN_COUNTRY_FLAG', 'FL_IS_CUSTOMS_OFFICE_OUTBOUND_FLAG', 'FL_IS_VEHICLE_ON_MARKET_COMPOUND_FLAG', 'FL_IS_VEHICLE_SOLD_TO_CUSTOMER_FLAG', 'FL_IS_VEHICLE_SOLD_FLAG', 'FL_IS_ANY_BLOCK_ACTIVE_FLAG', 'FL_IS_VEHICLE_FOR_SHOWROOM_FLAG', 'FL_IS_VEHICLE_FOR_DEMOSTRATION_FLAG', 'FL_IS_VEHICLE_IN_MARKET_COMPOUND_INVOICEABLE_FLAG']\n",
      "Dropped FL_ columns: ['FL_IS_VEHICLE_IN_MARKET_COMPOUND_INVOICEABLE_FLAG']\n",
      "Filled NaNs in 'FL_IS_DIRECT_SALES_FLAG' with its mode (0.0).\n",
      "Converted 31 FL_ columns to integer type.\n"
     ]
    }
   ],
   "source": [
    "# Identify all columns starting with \"FL_\" in the filtered DataFrame\n",
    "fl_cols_initial = filtered_df.columns[\n",
    "    filtered_df.columns.str.startswith(\"FL_\")\n",
    "].tolist()  #\n",
    "print(f\"Found {len(fl_cols_initial)} columns starting with 'FL_': {fl_cols_initial}\")\n",
    "\n",
    "# Define FL_ columns to be excluded/dropped\n",
    "excluded_fl_cols = [\n",
    "    \"FL_IS_VEHICLE_IN_MARKET_COMPOUND_INVOICEABLE_FLAG\",\n",
    "    # Add other FL_ columns to drop if identified as unnecessary\n",
    "]  #\n",
    "\n",
    "# Drop the excluded columns if they exist in the DataFrame\n",
    "actual_fl_cols_to_drop = [col for col in excluded_fl_cols if col in filtered_df.columns]\n",
    "if actual_fl_cols_to_drop:\n",
    "    filtered_df.drop(columns=actual_fl_cols_to_drop, inplace=True)  #\n",
    "    print(f\"Dropped FL_ columns: {actual_fl_cols_to_drop}\")\n",
    "\n",
    "# Update the list of FL_ columns to process\n",
    "fl_cols_to_process = [\n",
    "    col for col in fl_cols_initial if col not in actual_fl_cols_to_drop\n",
    "]  #\n",
    "\n",
    "# Handle missing values specifically for 'FL_IS_DIRECT_SALES_FLAG' by filling with mode\n",
    "direct_sales_col = \"FL_IS_DIRECT_SALES_FLAG\"\n",
    "if (\n",
    "    direct_sales_col in fl_cols_to_process\n",
    "    and filtered_df[direct_sales_col].isna().any()\n",
    "):\n",
    "    mode_direct_sales = filtered_df[direct_sales_col].mode()\n",
    "    if not mode_direct_sales.empty:\n",
    "        filtered_df[direct_sales_col].fillna(mode_direct_sales[0], inplace=True)  #\n",
    "        print(\n",
    "            f\"Filled NaNs in '{direct_sales_col}' with its mode ({mode_direct_sales[0]}).\"\n",
    "        )\n",
    "    else:\n",
    "        # Fallback if mode cannot be determined (e.g., all NaNs), fill with 0\n",
    "        filtered_df[direct_sales_col].fillna(0, inplace=True)\n",
    "        print(f\"Could not determine mode for '{direct_sales_col}', filled NaNs with 0.\")\n",
    "\n",
    "# For any other FL_ columns that might still have NaNs, a general fill (e.g., with 0) might be needed.\n",
    "# Here, we assume they should be convertible to int, implying NaNs should be handled.\n",
    "# For simplicity, let's fill remaining NaNs in FL_ columns with 0 beforeastype(int)\n",
    "for col in fl_cols_to_process:\n",
    "    if filtered_df[col].isna().any():\n",
    "        filtered_df[col].fillna(0, inplace=True)  # General fallback for other FL_ NaNs\n",
    "        print(f\"Filled remaining NaNs in '{col}' with 0.\")\n",
    "\n",
    "# Convert all processed FL_ columns to integer type (0 or 1)\n",
    "if fl_cols_to_process:\n",
    "    filtered_df[fl_cols_to_process] = filtered_df[fl_cols_to_process].astype(int)  #\n",
    "    print(f\"Converted {len(fl_cols_to_process)} FL_ columns to integer type.\")\n",
    "    # print(\"Data types of FL_ columns after processing:\")\n",
    "    # print(filtered_df[fl_cols_to_process].dtypes)\n",
    "else:\n",
    "    print(\"No FL_ columns to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_5_intro",
   "metadata": {},
   "source": [
    "## 5. Processing Categorical Code Columns (CD_)\n",
    "This section handles columns prefixed with `CD_`, representing categorical codes.\n",
    "The strategy involves:\n",
    "* Dropping columns that are entirely null.\n",
    "* Applying different imputation strategies based on the percentage of missing values:\n",
    "    * 0-5% missing: Impute with mode.\n",
    "    * 5-35% missing: Impute with a constant string \"UNK\" (Unknown).\n",
    "    * >35% missing: Drop column or apply specific manual imputation.\n",
    "* Creating flag columns to indicate where imputation occurred for mode/\"UNK\" filled columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cf69cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 columns starting with 'CD_'.\n",
      "Dropped CD_ columns with all null values: ['CD_VEHICLE_ORDER_CARFLOW_STATUS_CODE', 'CD_SOURCE_SYSTEM_VEHICLE_SHOWROOM_STATUS_CODE']\n",
      "Imputed 'CD_NETWORK_SUBJECT_CODE' with 'GBW1350' (or mode) and created flag 'is_missing_CD_NETWORK_SUBJECT_CODE'.\n",
      "Imputed 'CD_FINAL_CUSTOMER_ORDER_CODE' with '220000233' (or mode) and created flag 'is_missing_CD_FINAL_CUSTOMER_ORDER_CODE'.\n",
      "Imputed 'CD_NETWORK_SUBJECT_FOR_INVOICING_CODE' with 'GBW1350' (or mode) and created flag 'is_missing_CD_NETWORK_SUBJECT_FOR_INVOICING_CODE'.\n",
      "Imputed 'CD_SALES_REGION_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_SALES_REGION_CODE'.\n",
      "Imputed 'CD_SHIPPING_ORDER_TO_NSC_COMPOUND_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_SHIPPING_ORDER_TO_NSC_COMPOUND_CODE'.\n",
      "Imputed 'CD_VEHICLE_ORDER_ORIGIN_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_VEHICLE_ORDER_ORIGIN_CODE'.\n",
      "Imputed 'CD_VEHICLE_LAST_LOCATION_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_VEHICLE_LAST_LOCATION_CODE'.\n",
      "Imputed 'CD_VEHICLE_LAST_LOCATION_TYPE_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_VEHICLE_LAST_LOCATION_TYPE_CODE'.\n",
      "Imputed 'CD_SHIPPING_ZONE_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_SHIPPING_ZONE_CODE'.\n",
      "Imputed 'CD_CUSTOMER_SALES_CHANNEL_ORIGINAL_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_CUSTOMER_SALES_CHANNEL_ORIGINAL_CODE'.\n",
      "Imputed 'CD_CUSTOMER_SALES_SUBCHANNEL_ORIGINAL_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_CUSTOMER_SALES_SUBCHANNEL_ORIGINAL_CODE'.\n",
      "Imputed 'CD_DISTRIBUTION_SALES_CHANNEL_ORIGINAL_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_DISTRIBUTION_SALES_CHANNEL_ORIGINAL_CODE'.\n",
      "Imputed 'CD_OPERATIONAL_PLANNING_SALES_CHANNEL_ORIGINAL_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_OPERATIONAL_PLANNING_SALES_CHANNEL_ORIGINAL_CODE'.\n",
      "Imputed 'CD_VEHICLE_TRANSPORTATION_CURRENT_STATUS_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_VEHICLE_TRANSPORTATION_CURRENT_STATUS_CODE'.\n",
      "Imputed 'CD_SHIPPER_DEALER_CODE' with 'UNK' (or mode) and created flag 'is_missing_CD_SHIPPER_DEALER_CODE'.\n",
      "Dropped CD_ columns with high NA (>35% or specifically listed): ['CD_VEHICLE_ORDER_CODE', 'CD_SHIPPING_ORDER_TO_DEALER_CODE', 'CD_SOURCE_SYSTEM_BLOCK_REASON_TYPE', 'CD_SOURCE_SYSTEM_BLOCK_REASON_CODE', 'CD_GLOBAL_FINAL_CUSTOMER_ORDER_CODE']\n",
      "Filled NaNs in 'CD_NETWORK_SUBJECT_ORDER_CODE' with 'NOT_DEALER_ORDER'.\n",
      "Filled NaNs in 'CD_OPERATIONAL_PLANNING_SALES_SUBCHANNEL_ORIGINAL_CODE' with 'UNK' and created flag.\n",
      "\n",
      "CD_ columns processed. Check for remaining NaNs:\n",
      "CD_DESTINATION_LOGISTIC_COMPOUND_CODE                 5189\n",
      "CD_DESTINATION_LOGISTIC_COMPOUND_TYPE_CODE            5189\n",
      "CD_SOURCE_SYSTEM_VEHICLE_DEMONSTRATION_STATUS_CODE    4654\n",
      "CD_DISTRIBUTION_SALES_SUBCHANNEL_ORIGINAL_CODE        4922\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify columns starting with \"CD_\"\n",
    "code_cols_initial = filtered_df.columns[\n",
    "    filtered_df.columns.str.startswith(\"CD_\")\n",
    "].tolist()  #\n",
    "print(f\"Found {len(code_cols_initial)} columns starting with 'CD_'.\")\n",
    "\n",
    "# Drop CD_ columns that are 100% null\n",
    "cols_to_drop_all_null = [\n",
    "    col for col in code_cols_initial if filtered_df[col].isna().all()\n",
    "]  #\n",
    "if cols_to_drop_all_null:\n",
    "    filtered_df.drop(columns=cols_to_drop_all_null, inplace=True)\n",
    "    print(f\"Dropped CD_ columns with all null values: {cols_to_drop_all_null}\")\n",
    "\n",
    "# Update the list of CD_ columns after dropping all-null ones\n",
    "code_cols_to_process = filtered_df.columns[\n",
    "    filtered_df.columns.str.startswith(\"CD_\")\n",
    "].tolist()  #\n",
    "\n",
    "# Calculate the percentage of missing values for the remaining CD_ columns\n",
    "na_percentage_cd = (\n",
    "    filtered_df[code_cols_to_process].isna().mean() * 100\n",
    "    if code_cols_to_process\n",
    "    else pd.Series(dtype=float)\n",
    ")  #\n",
    "\n",
    "\n",
    "# Helper function for imputation and adding a missing flag\n",
    "def impute_with_flag(df, column_name, fill_value_or_func):\n",
    "    \"\"\"Imputes missing values and adds a flag column.\"\"\"\n",
    "    flag_col_name = f\"is_missing_{column_name}\"\n",
    "    df[flag_col_name] = df[column_name].isna().astype(int)  #\n",
    "\n",
    "    fill_value = fill_value_or_func\n",
    "    if callable(fill_value_or_func):\n",
    "        # Check if series is all NaN before calling mode, to avoid empty mode result\n",
    "        if df[column_name].notna().any():\n",
    "            fill_value = fill_value_or_func(df, column_name)\n",
    "        else:  # If all NaN, mode is empty, fill with a default like \"UNK\"\n",
    "            fill_value = \"UNK_ALL_NAN\"\n",
    "            print(\n",
    "                f\"Warning: Column {column_name} is all NaN. Imputing with '{fill_value}'.\"\n",
    "            )\n",
    "\n",
    "    df[column_name].fillna(fill_value, inplace=True)  #\n",
    "    print(\n",
    "        f\"Imputed '{column_name}' with '{fill_value}' (or mode) and created flag '{flag_col_name}'.\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# Columns with 0% < NA < 5%: Impute with mode\n",
    "cols_impute_mode = na_percentage_cd[\n",
    "    (na_percentage_cd > 0) & (na_percentage_cd < 5)\n",
    "].index.tolist()  #\n",
    "for col in cols_impute_mode:\n",
    "    filtered_df = impute_with_flag(\n",
    "        filtered_df,\n",
    "        col,\n",
    "        lambda df_lambda, c: df_lambda[c].mode()[0]\n",
    "        if not df_lambda[c].mode().empty\n",
    "        else \"UNK_MODE_EMPTY\",\n",
    "    )\n",
    "\n",
    "# Columns with 5% <= NA < 20%: Impute with \"UNK\"\n",
    "cols_impute_unk_medium = na_percentage_cd[\n",
    "    (na_percentage_cd >= 5) & (na_percentage_cd < 20)\n",
    "].index.tolist()  #\n",
    "for col in cols_impute_unk_medium:\n",
    "    filtered_df = impute_with_flag(filtered_df, col, \"UNK\")\n",
    "\n",
    "# Columns with 20% <= NA < 35%: Impute with \"UNK\"\n",
    "cols_impute_unk_high = na_percentage_cd[\n",
    "    (na_percentage_cd >= 20) & (na_percentage_cd < 35)\n",
    "].index.tolist()  #\n",
    "for col in cols_impute_unk_high:\n",
    "    filtered_df = impute_with_flag(filtered_df, col, \"UNK\")\n",
    "\n",
    "# Columns with NA >= 35%: Drop these columns as per original script's specific list\n",
    "cols_to_drop_high_na_specific = [\n",
    "    \"CD_VEHICLE_ORDER_CODE\",\n",
    "    \"CD_SHIPPING_ORDER_TO_DEALER_CODE\",\n",
    "    \"CD_SOURCE_SYSTEM_BLOCK_REASON_TYPE\",\n",
    "    \"CD_SOURCE_SYSTEM_BLOCK_REASON_CODE\",\n",
    "    \"CD_GLOBAL_FINAL_CUSTOMER_ORDER_CODE\",\n",
    "]  #\n",
    "actual_cols_to_drop_high_na = [\n",
    "    col for col in cols_to_drop_high_na_specific if col in filtered_df.columns\n",
    "]\n",
    "if actual_cols_to_drop_high_na:\n",
    "    filtered_df.drop(columns=actual_cols_to_drop_high_na, inplace=True)\n",
    "    print(\n",
    "        f\"Dropped CD_ columns with high NA (>35% or specifically listed): {actual_cols_to_drop_high_na}\"\n",
    "    )\n",
    "\n",
    "# Specific manual imputation for certain CD_ columns (example from original script)\n",
    "if \"CD_NETWORK_SUBJECT_ORDER_CODE\" in filtered_df.columns:\n",
    "    if filtered_df[\"CD_NETWORK_SUBJECT_ORDER_CODE\"].isna().any():\n",
    "        filtered_df[\"CD_NETWORK_SUBJECT_ORDER_CODE\"].fillna(\n",
    "            \"NOT_DEALER_ORDER\", inplace=True\n",
    "        )  #\n",
    "        print(\"Filled NaNs in 'CD_NETWORK_SUBJECT_ORDER_CODE' with 'NOT_DEALER_ORDER'.\")\n",
    "\n",
    "subchannel_col = \"CD_OPERATIONAL_PLANNING_SALES_SUBCHANNEL_ORIGINAL_CODE\"\n",
    "if subchannel_col in filtered_df.columns:\n",
    "    if filtered_df[subchannel_col].isna().any():\n",
    "        # This column was listed with >35% NA in the original script, but then manually handled.\n",
    "        # Re-applying the specific logic from the script if it wasn't dropped above.\n",
    "        filtered_df[f\"is_missing_{subchannel_col}\"] = (\n",
    "            filtered_df[subchannel_col].isna().astype(int)\n",
    "        )  #\n",
    "        filtered_df[subchannel_col].fillna(\"UNK\", inplace=True)  #\n",
    "        print(f\"Filled NaNs in '{subchannel_col}' with 'UNK' and created flag.\")\n",
    "\n",
    "print(\"\\nCD_ columns processed. Check for remaining NaNs:\")\n",
    "final_cd_cols = filtered_df.columns[filtered_df.columns.str.startswith(\"CD_\")]\n",
    "if not final_cd_cols.empty:\n",
    "    print(filtered_df[final_cd_cols].isna().sum().loc[lambda x: x > 0])\n",
    "else:\n",
    "    print(\"No CD_ columns remaining or all are handled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_6_intro",
   "metadata": {},
   "source": [
    "## 6. Processing General Text Columns (GN_)\n",
    "This section handles columns prefixed with `GN_`, which are general text or name columns.\n",
    "Steps include:\n",
    "* Dropping columns that are entirely null.\n",
    "* Dropping columns containing 'GUID' in their name (unique identifiers, usually not useful as features).\n",
    "* Dropping a specific list of GN_ columns deemed unnecessary or having too many missing values.\n",
    "* Imputing `GN_BRAND_ORIGINAL_NAME` using a mapping from `CD_BRAND_CODE`.\n",
    "* Imputing `GN_NETWORK_SUBJECT_NAME` with \"UNK\".\n",
    "* Imputing `GN_VEHICLE_CAR_LINE_NAME` with its mode (conditional on `CD_BRAND_CODE` == \"C\" in original script, generalized here to overall mode if specific mode fails).\n",
    "* Converting remaining GN_ columns (that were object type) to string type for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd24c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 columns starting with 'GN_'.\n",
      "Dropped GN_ columns with all null values: ['GN_VEHICLE_RENT_A_CAR_FLEET_DOCUMENTS_DISPATCH_TRACKING_CODE']\n",
      "Dropped GN_ columns containing 'GUID': ['GN_SYSTEM_ORDER_GUID', 'GN_OPERATIONAL_PLANNING_SALES_CHANNEL_GUID', 'GN_OPERATIONAL_PLANNING_SALES_SUBCHANNEL_GUID', 'GN_NETWORK_SUBJECT_FOR_INVOICING_GUID', 'GN_VEHICLE_ORDER_GUID', 'GN_FINAL_CUSTOMER_ORDER_GUID', 'GN_DESTINATION_LOGISTIC_COMPOUND_GUID', 'GN_DESTINATION_LOGISTIC_COMPOUND_TYPE_GUID', 'GN_NETWORK_SUBJECT_ORDER_GUID', 'GN_CUSTOMER_SALES_CHANNEL_GUID', 'GN_CUSTOMER_SALES_SUBCHANNEL_GUID', 'GN_DISTRIBUTION_SALES_CHANNEL_GUID', 'GN_DISTRIBUTION_SALES_SUBCHANNEL_GUID']\n",
      "\n",
      "NA percentage in remaining GN_ columns before further dropping/imputation:\n",
      "GN_BRAND_ORIGINAL_NAME                                    54.920803\n",
      "GN_OPERATIONAL_PLANNING_SALES_CHANNEL_ORIGINAL_NAME       23.072862\n",
      "GN_OPERATIONAL_PLANNING_SALES_CHANNEL_NAME                58.152059\n",
      "GN_OPERATIONAL_PLANNING_SALES_SUBCHANNEL_ORIGINAL_NAME    38.109820\n",
      "GN_NETWORK_SUBJECT_NAME                                    4.329461\n",
      "GN_SALES_INVOICE_NUMBER                                   55.860612\n",
      "GN_VEHICLE_ORDER_SECTION_NUMBER                           98.489968\n",
      "GN_INCOTERM_OUTBOUND_FLOW                                 99.672650\n",
      "GN_VEHICLE_ORDER_STATUS_DETAIL_NAME                       21.953537\n",
      "GN_VEHICLE_LAST_LOCATION_NAME                             19.904963\n",
      "GN_SHIPPER_DEALER_NAME                                    20.021119\n",
      "GN_SHIPPING_ZONE_NAME                                     19.915523\n",
      "GN_DESTINATION_LOGISTIC_COMPOUND_NAME                     54.794087\n",
      "GN_SOURCE_SYSTEM_BLOCK_REASON_NAME                        97.507920\n",
      "GN_CUSTOMER_SALES_CHANNEL_ORIGINAL_NAME                   48.215417\n",
      "GN_CUSTOMER_SALES_CHANNEL_NAME                             7.455121\n",
      "GN_CUSTOMER_SALES_SUBCHANNEL_ORIGINAL_NAME                49.271383\n",
      "GN_DISTRIBUTION_SALES_CHANNEL_ORIGINAL_NAME               51.414995\n",
      "GN_DISTRIBUTION_SALES_SUBCHANNEL_ORIGINAL_NAME            51.974657\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Select initial GN_ columns\n",
    "gn_cols_initial = filtered_df.columns[\n",
    "    filtered_df.columns.str.startswith(\"GN_\")\n",
    "].tolist()  #\n",
    "print(f\"Found {len(gn_cols_initial)} columns starting with 'GN_'.\")\n",
    "\n",
    "# Drop GN_ columns that are 100% empty (all NaN)\n",
    "empty_gn_cols = [col for col in gn_cols_initial if filtered_df[col].isna().all()]  #\n",
    "if empty_gn_cols:\n",
    "    filtered_df.drop(columns=empty_gn_cols, inplace=True)\n",
    "    print(f\"Dropped GN_ columns with all null values: {empty_gn_cols}\")\n",
    "\n",
    "# Drop GN_ columns containing 'GUID' in their name\n",
    "guid_gn_cols = filtered_df.columns[\n",
    "    filtered_df.columns.str.contains(\"GUID\") & filtered_df.columns.str.startswith(\"GN_\")\n",
    "].tolist()  #\n",
    "if guid_gn_cols:\n",
    "    filtered_df.drop(columns=guid_gn_cols, inplace=True)\n",
    "    print(f\"Dropped GN_ columns containing 'GUID': {guid_gn_cols}\")\n",
    "\n",
    "# Update the list of GN_ columns to process\n",
    "gn_cols_to_process = filtered_df.columns[\n",
    "    filtered_df.columns.str.startswith(\"GN_\")\n",
    "].tolist()\n",
    "\n",
    "# Display NA count for remaining GN_ columns (informational, as in original script)\n",
    "if gn_cols_to_process:\n",
    "    print(\n",
    "        \"\\nNA percentage in remaining GN_ columns before further dropping/imputation:\"\n",
    "    )\n",
    "    na_counts_gn = filtered_df[gn_cols_to_process].isna().mean() * 100  #\n",
    "    print(na_counts_gn[na_counts_gn > 0])\n",
    "else:\n",
    "    print(\"No GN_ columns remaining after initial drops.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ca911db-9e4e-4bf9-ade2-f777dcd06a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped specifically listed GN_ columns: ['GN_OPERATIONAL_PLANNING_SALES_CHANNEL_ORIGINAL_NAME', 'GN_OPERATIONAL_PLANNING_SALES_CHANNEL_NAME', 'GN_OPERATIONAL_PLANNING_SALES_SUBCHANNEL_ORIGINAL_NAME', 'GN_SALES_INVOICE_NUMBER', 'GN_VEHICLE_ORDER_SECTION_NUMBER', 'GN_INCOTERM_OUTBOUND_FLOW', 'GN_VEHICLE_ORDER_STATUS_DETAIL_NAME', 'GN_VEHICLE_LAST_LOCATION_NAME', 'GN_SHIPPER_DEALER_NAME', 'GN_SHIPPING_ZONE_NAME', 'GN_DESTINATION_LOGISTIC_COMPOUND_NAME', 'GN_SOURCE_SYSTEM_BLOCK_REASON_NAME', 'GN_CUSTOMER_SALES_CHANNEL_ORIGINAL_NAME', 'GN_DISTRIBUTION_SALES_CHANNEL_ORIGINAL_NAME', 'GN_DISTRIBUTION_SALES_SUBCHANNEL_ORIGINAL_NAME', 'GN_CUSTOMER_SALES_SUBCHANNEL_ORIGINAL_NAME']\n"
     ]
    }
   ],
   "source": [
    "# Specific list of GN_ columns to drop (based on original script's decision, likely due to high NAs or low relevance)\n",
    "gn_cols_to_drop_specific = [\n",
    "    \"GN_OPERATIONAL_PLANNING_SALES_CHANNEL_ORIGINAL_NAME\",\n",
    "    \"GN_OPERATIONAL_PLANNING_SALES_CHANNEL_NAME\",\n",
    "    \"GN_OPERATIONAL_PLANNING_SALES_SUBCHANNEL_ORIGINAL_NAME\",\n",
    "    \"GN_SALES_INVOICE_NUMBER\",\n",
    "    \"GN_VEHICLE_ORDER_SECTION_NUMBER\",\n",
    "    \"GN_INCOTERM_OUTBOUND_FLOW\",\n",
    "    \"GN_VEHICLE_ORDER_STATUS_DETAIL_NAME\",\n",
    "    \"GN_VEHICLE_LAST_LOCATION_NAME\",\n",
    "    \"GN_SHIPPER_DEALER_NAME\",\n",
    "    \"GN_SHIPPING_ZONE_NAME\",\n",
    "    \"GN_DESTINATION_LOGISTIC_COMPOUND_NAME\",\n",
    "    \"GN_SOURCE_SYSTEM_BLOCK_REASON_NAME\",\n",
    "    \"GN_CUSTOMER_SALES_CHANNEL_ORIGINAL_NAME\",\n",
    "    # 'GN_CUSTOMER_SALES_CHANNEL_NAME', # This was kept in some versions, verify necessity\n",
    "    \"GN_DISTRIBUTION_SALES_CHANNEL_ORIGINAL_NAME\",\n",
    "    \"GN_DISTRIBUTION_SALES_SUBCHANNEL_ORIGINAL_NAME\",\n",
    "    \"GN_CUSTOMER_SALES_SUBCHANNEL_ORIGINAL_NAME\",\n",
    "]  #\n",
    "\n",
    "actual_gn_cols_to_drop = [\n",
    "    col for col in gn_cols_to_drop_specific if col in filtered_df.columns\n",
    "]\n",
    "if actual_gn_cols_to_drop:\n",
    "    filtered_df.drop(columns=actual_gn_cols_to_drop, inplace=True)  #\n",
    "    print(f\"Dropped specifically listed GN_ columns: {actual_gn_cols_to_drop}\")\n",
    "\n",
    "# Update gn_cols_to_process again\n",
    "gn_cols_to_process = filtered_df.columns[\n",
    "    filtered_df.columns.str.startswith(\"GN_\")\n",
    "].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afab045a-54aa-409b-b8ce-e6bd0364e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping for GN_BRAND_ORIGINAL_NAME imputation from CD_BRAND_CODE\n",
    "brand_code_to_name_map = {\n",
    "    \"B\": \"Abarth\",\n",
    "    \"A\": \"Alfa Romeo\",\n",
    "    \"L\": \"Chrysler\",\n",
    "    \"W\": \"Dodge\",\n",
    "    \"I\": \"Fiat\",\n",
    "    \"J\": \"Jeep\",\n",
    "    \"E\": \"Lancia\",\n",
    "    \"O\": \"RAM\",\n",
    "    \"X\": \"Maserati\",\n",
    "    \"C\": \"Citroën\",\n",
    "    \"P\": \"Peugeot\",\n",
    "    \"S\": \"DS\",\n",
    "    \"G\": \"Opel\",\n",
    "}  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4127510-dbd6-41a8-8f96-2e0b1d32e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed missing values in 'GN_BRAND_ORIGINAL_NAME' using 'CD_BRAND_CODE' mapping and 'Unknown_Brand' as fallback.\n"
     ]
    }
   ],
   "source": [
    "# Impute 'GN_BRAND_ORIGINAL_NAME' where it's missing, using the mapping\n",
    "brand_name_col = \"GN_BRAND_ORIGINAL_NAME\"\n",
    "brand_code_col = \"CD_BRAND_CODE\"\n",
    "if brand_name_col in filtered_df.columns and brand_code_col in filtered_df.columns:\n",
    "    if filtered_df[brand_name_col].isna().any():\n",
    "        # Create a series of mapped names from the brand code\n",
    "        mapped_brand_names = filtered_df[brand_code_col].map(brand_code_to_name_map)  #\n",
    "        # Fill NaNs in GN_BRAND_ORIGINAL_NAME with these mapped names\n",
    "        filtered_df[brand_name_col] = filtered_df[brand_name_col].fillna(\n",
    "            mapped_brand_names\n",
    "        )  #\n",
    "        # For any remaining NaNs (if CD_BRAND_CODE was NaN or not in map), fill with \"Unknown_Brand\"\n",
    "        if filtered_df[brand_name_col].isna().any():\n",
    "            filtered_df[brand_name_col].fillna(\"Unknown_Brand\", inplace=True)\n",
    "        print(\n",
    "            f\"Imputed missing values in '{brand_name_col}' using '{brand_code_col}' mapping and 'Unknown_Brand' as fallback.\"\n",
    "        )\n",
    "else:\n",
    "    print(\n",
    "        f\"Skipping imputation for '{brand_name_col}' as it or '{brand_code_col}' is not in DataFrame.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f3abb9a-30fb-4cff-abeb-b2562fbae8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 'GN_NETWORK_SUBJECT_NAME' with 'UNK' (or mode) and created flag 'is_missing_GN_NETWORK_SUBJECT_NAME'.\n"
     ]
    }
   ],
   "source": [
    "# Impute 'GN_NETWORK_SUBJECT_NAME' with \"UNK\" and add a flag (using the helper function)\n",
    "network_subject_col = \"GN_NETWORK_SUBJECT_NAME\"\n",
    "if (\n",
    "    network_subject_col in filtered_df.columns\n",
    "    and filtered_df[network_subject_col].isna().any()\n",
    "):\n",
    "    filtered_df = impute_with_flag(filtered_df, network_subject_col, \"UNK\")  #\n",
    "else:\n",
    "    if network_subject_col not in filtered_df.columns:\n",
    "        print(f\"Column '{network_subject_col}' not found for imputation.\")\n",
    "    else:\n",
    "        print(f\"No missing values in '{network_subject_col}' to impute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "427f91b4-63b2-486a-9399-8ab02b2b9f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in 'GN_VEHICLE_CAR_LINE_NAME' to impute.\n"
     ]
    }
   ],
   "source": [
    "# Impute 'GN_VEHICLE_CAR_LINE_NAME'\n",
    "# Original script had specific logic: df[df[\"CD_BRAND_CODE\"] == \"C\"][\"GN_VEHICLE_CAR_LINE_NAME\"].mode().to_list()[0]\n",
    "# This implies filling NaNs with the mode of car line names for Citroën (\"C\") vehicles.\n",
    "# We'll generalize: if 'C' exists and has a mode, use it. Otherwise, use overall mode, then \"Unknown_Car_Line\".\n",
    "car_line_col = \"GN_VEHICLE_CAR_LINE_NAME\"\n",
    "if car_line_col in filtered_df.columns and filtered_df[car_line_col].isna().any():\n",
    "    fill_value_car_line = \"Unknown_Car_Line\"  # Default fallback\n",
    "    if (\n",
    "        brand_code_col in filtered_df.columns\n",
    "        and \"C\" in filtered_df[brand_code_col].unique()\n",
    "    ):\n",
    "        mode_for_C = filtered_df[filtered_df[brand_code_col] == \"C\"][\n",
    "            car_line_col\n",
    "        ].mode()\n",
    "        if not mode_for_C.empty:\n",
    "            fill_value_car_line = mode_for_C[0]\n",
    "            print(\n",
    "                f\"Using mode for brand 'C' for '{car_line_col}': {fill_value_car_line}\"\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to overall mode if brand 'C' has no mode for car line\n",
    "            overall_mode = filtered_df[car_line_col].mode()\n",
    "            if not overall_mode.empty:\n",
    "                fill_value_car_line = overall_mode[0]\n",
    "                print(f\"Using overall mode for '{car_line_col}': {fill_value_car_line}\")\n",
    "    else:\n",
    "        # Fallback to overall mode if brand 'C' not present or CD_BRAND_CODE missing\n",
    "        overall_mode = filtered_df[car_line_col].mode()\n",
    "        if not overall_mode.empty:\n",
    "            fill_value_car_line = overall_mode[0]\n",
    "            print(f\"Using overall mode for '{car_line_col}': {fill_value_car_line}\")\n",
    "\n",
    "    filtered_df[car_line_col].fillna(fill_value_car_line, inplace=True)  #\n",
    "    print(f\"Filled NaNs in '{car_line_col}' with '{fill_value_car_line}'.\")\n",
    "else:\n",
    "    if car_line_col not in filtered_df.columns:\n",
    "        print(f\"Column '{car_line_col}' not found for imputation.\")\n",
    "    else:\n",
    "        print(f\"No missing values in '{car_line_col}' to impute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be3edf2c-91bd-449f-9753-8d27978a25ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted specified GN_ columns to 'string' type.\n",
      "\n",
      "Data types of GN_ columns after processing:\n",
      "GN_BRAND_ORIGINAL_NAME                  string[python]\n",
      "GN_VEHICLE_COMPLETE_DESCRIPTION_NAME    string[python]\n",
      "GN_VEHICLE_CAR_LINE_NAME                string[python]\n",
      "GN_NETWORK_SUBJECT_NAME                 string[python]\n",
      "GN_VEHICLE_ORDER_FLOW_TYPE                       int64\n",
      "GN_CUSTOMER_SALES_CHANNEL_NAME                  object\n",
      "dtype: object\n",
      "\n",
      "Missing values in GN_ columns after processing:\n",
      "GN_CUSTOMER_SALES_CHANNEL_NAME    706\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert remaining GN_ columns (that were object and are now processed) to string type\n",
    "# This ensures consistency and avoids issues with mixed types if any imputation resulted in numbers/bools etc.\n",
    "final_gn_cols_to_convert = [\n",
    "    \"GN_BRAND_ORIGINAL_NAME\",\n",
    "    \"GN_VEHICLE_COMPLETE_DESCRIPTION_NAME\",\n",
    "    \"GN_VEHICLE_CAR_LINE_NAME\",\n",
    "    \"GN_NETWORK_SUBJECT_NAME\",\n",
    "    # Add any other GN_ columns that should be string and were kept\n",
    "]  #\n",
    "\n",
    "for col in final_gn_cols_to_convert:\n",
    "    if col in filtered_df.columns:\n",
    "        # Fill any potential post-imputation NaNs (e.g. if a mode was NaN) with a placeholder before converting to string\n",
    "        if filtered_df[col].isna().any():\n",
    "            filtered_df[col].fillna(\"UNKNOWN_GN_VALUE\", inplace=True)\n",
    "        filtered_df[col] = filtered_df[col].astype(\"string\")  #\n",
    "\n",
    "print(f\"Converted specified GN_ columns to 'string' type.\")\n",
    "gn_cols_final_check = filtered_df.columns[filtered_df.columns.str.startswith(\"GN_\")]\n",
    "if not gn_cols_final_check.empty:\n",
    "    print(\"\\nData types of GN_ columns after processing:\")\n",
    "    print(filtered_df[gn_cols_final_check].dtypes)  #\n",
    "    print(\"\\nMissing values in GN_ columns after processing:\")\n",
    "    print(filtered_df[gn_cols_final_check].isna().sum().loc[lambda x: x > 0])\n",
    "else:\n",
    "    print(\"No GN_ columns remaining.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_7_intro",
   "metadata": {},
   "source": [
    "## 7. Processing Numerical Columns (NM_)\n",
    "This section handles columns prefixed with `NM_`, which are numerical.\n",
    "The original script drops all `NM_` columns. This step is retained here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d2461c4-45dc-48f2-937b-59a5bbd98036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped all NM_ columns: ['NM_VEHICLE_ORDER_OEM_STOCK_AGEING_NUMBER', 'NM_VEHICLE_ORDER_DEALER_STOCK_AGEING_NUMBER', 'NM_NEW_VEHICLE_ORDER_GENERAL_DISTRIBUTOR_STOCK_AGEING_NUMBER']\n"
     ]
    }
   ],
   "source": [
    "# Identify columns starting with \"NM_\"\n",
    "nm_cols = filtered_df.columns[filtered_df.columns.str.startswith(\"NM_\")].tolist()  #\n",
    "\n",
    "if nm_cols:\n",
    "    filtered_df.drop(columns=nm_cols, inplace=True)  #\n",
    "    print(f\"Dropped all NM_ columns: {nm_cols}\")\n",
    "else:\n",
    "    print(\"No NM_ columns found to drop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_8_intro",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering\n",
    "This section creates new features from existing columns to potentially improve model performance or provide richer insights.\n",
    "Features created include:\n",
    "* `Is_Delayed`: Binary flag indicating if the delivery was delayed.\n",
    "* `REG_Delay_Days`: Delay in days, clipped at 0 (negative delays treated as 0).\n",
    "* Various time differences between key process stages (e.g., `ORDER_TO_PRODUCTION_DAYS`).\n",
    "* `CUSTOMS_CLEARANCE_DAYS`: Duration of customs clearance (NaN for domestic).\n",
    "* `TOTAL_LEAD_VS_EXPECTED_DAYS`: Difference between actual and expected total lead time.\n",
    "* Flags for missing key dates (redundant if imputation flags already exist, but kept per original script).\n",
    "* Date components like `ORDER_DAY_OF_WEEK` and `PRODUCTION_MONTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "908f9cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'Is_Delayed' feature.\n"
     ]
    }
   ],
   "source": [
    "# Create 'Is_Delayed' flag (binary: 1 if delayed, 0 otherwise)\n",
    "# 'Delay_days' is a Timedelta object, so we access .dt.days\n",
    "if \"Delay_days\" in filtered_df.columns:\n",
    "    filtered_df[\"Is_Delayed\"] = (filtered_df[\"Delay_days\"].dt.days > 0).astype(int)  #\n",
    "    print(\"Created 'Is_Delayed' feature.\")\n",
    "else:\n",
    "    print(\n",
    "        \"Warning: 'Delay_days' column not found. 'Is_Delayed' feature cannot be created.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "873fc9d0-3ea8-4d8c-b518-5842eb977d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'REG_Delay_Days' feature.\n",
      "Dropped original 'Delay_days' column.\n"
     ]
    }
   ],
   "source": [
    "# Create 'REG_Delay_Days' - delay in days, with negative values clipped to 0 (no \"early\" credit)\n",
    "if \"Delay_days\" in filtered_df.columns:\n",
    "    filtered_df[\"REG_Delay_Days\"] = filtered_df[\"Delay_days\"].dt.days.clip(lower=0)  #\n",
    "    print(\"Created 'REG_Delay_Days' feature.\")\n",
    "\n",
    "    # Drop the original 'Delay_days' (Timedelta object) as its information is now in 'Is_Delayed' and 'REG_Delay_Days'\n",
    "    filtered_df.drop(columns=[\"Delay_days\"], inplace=True)  #\n",
    "    print(\"Dropped original 'Delay_days' column.\")\n",
    "else:\n",
    "    print(\n",
    "        \"Warning: 'Delay_days' column not found. 'REG_Delay_Days' feature cannot be created and 'Delay_days' cannot be dropped.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "551c5137-3e87-4019-a92c-e9c250b34c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time from commercial order to factory production\n",
    "# if \"DT_VEHICLE_FACTORY_PRODUCTION_DATE\" in filtered_df.columns and \\\n",
    "#   \"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\" in filtered_df.columns:\n",
    "#    filtered_df[\"ORDER_TO_PRODUCTION_DAYS\"] = (\n",
    "#        filtered_df[\"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"]\n",
    "#        - filtered_df[\"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\"]\n",
    "#    ).dt.days #\n",
    "#    print(\"Created 'ORDER_TO_PRODUCTION_DAYS' feature.\")\n",
    "# else:\n",
    "#    print(\"Warning: Required date columns missing for 'ORDER_TO_PRODUCTION_DAYS'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b92df7d-ad76-4492-8a07-ad8da3386024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time from factory production to ready to ship from logistic plant\n",
    "# if \"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\" in filtered_df.columns and \\\n",
    "#   \"DT_VEHICLE_FACTORY_PRODUCTION_DATE\" in filtered_df.columns:\n",
    "#    filtered_df[\"PRODUCTION_TO_SHIPPING_READY_DAYS\"] = (\n",
    "#        filtered_df[\"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\"]\n",
    "#        - filtered_df[\"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"]\n",
    "#    ).dt.days #\n",
    "#    print(\"Created 'PRODUCTION_TO_SHIPPING_READY_DAYS' feature.\")\n",
    "# else:\n",
    "#    print(\"Warning: Required date columns missing for 'PRODUCTION_TO_SHIPPING_READY_DAYS'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "157a706a-d839-48e5-ad21-2fb3c6a4b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time from ready to ship to shipping order creation (commented out in original script, retained as such)\n",
    "# if \"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\" in filtered_df.columns and \\\n",
    "#    \"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\" in filtered_df.columns:\n",
    "#     filtered_df[\"SHIPPING_READY_TO_SHIPPING_ORDER_DAYS\"] = (\n",
    "#         filtered_df[\"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\"]\n",
    "#         - filtered_df[\"DT_READY_TO_SHIP_FROM_LOGISTIC_PLANT_COMPOUND_DATE\"]\n",
    "#     ).dt.days #\n",
    "#     print(\"Created 'SHIPPING_READY_TO_SHIPPING_ORDER_DAYS' feature (if uncommented).\")\n",
    "# else:\n",
    "#     print(\"Warning: Required date columns missing for 'SHIPPING_READY_TO_SHIPPING_ORDER_DAYS' (if uncommented).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e43c8110-54c2-485c-97bc-868b49f14b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time from shipping order creation to arrival at destination by transporter\n",
    "# if \"DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE\" in filtered_df.columns and \\\n",
    "#   \"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\" in filtered_df.columns:\n",
    "#    filtered_df[\"SHIPPING_TRANSIT_DAYS\"] = (\n",
    "#        filtered_df[\"DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE\"]\n",
    "#        - filtered_df[\"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\"]\n",
    "#    ).dt.days #\n",
    "#    print(\"Created 'SHIPPING_TRANSIT_DAYS' feature.\")\n",
    "# else:\n",
    "#    print(\"Warning: Required date columns missing for 'SHIPPING_TRANSIT_DAYS'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78b805b6-9805-43ce-9f9a-b8c5409a3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customs clearance time\n",
    "# if \"DT_CUSTOMS_OFFICE_OUTBOUND_DATE\" in filtered_df.columns and \\\n",
    "#   \"DT_CUSTOMS_OFFICE_INBOUND_DATE\" in filtered_df.columns and \\\n",
    "#   \"IS_INTERNATIONAL\" in filtered_df.columns:\n",
    "#    filtered_df[\"CUSTOMS_CLEARANCE_DAYS\"] = (\n",
    "#        filtered_df[\"DT_CUSTOMS_OFFICE_OUTBOUND_DATE\"]\n",
    "#        - filtered_df[\"DT_CUSTOMS_OFFICE_INBOUND_DATE\"]\n",
    "#    ).dt.days #\n",
    "\n",
    "#    # Set to NaN (or a specific value like -1 or 0) for domestic shipments where customs dates are not applicable\n",
    "#    filtered_df.loc[filtered_df[\"IS_INTERNATIONAL\"] == 0, \"CUSTOMS_CLEARANCE_DAYS\"] = np.nan #\n",
    "#    print(\"Created 'CUSTOMS_CLEARANCE_DAYS' feature (NaN for domestic).\")\n",
    "# else:\n",
    "#    print(\"Warning: Required columns missing for 'CUSTOMS_CLEARANCE_DAYS'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a103639d-7903-4767-9402-f7c02c79bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between actual total lead time and expected total lead time\n",
    "# if \"DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE\" in filtered_df.columns and \\\n",
    "#   \"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\" in filtered_df.columns and \\\n",
    "#   \"DT_EXPECTED_DELIVERY_TO_FINAL_CUSTOMER_DATE\" in filtered_df.columns:\n",
    "#    actual_lead_time_days = (\n",
    "#        filtered_df[\"DT_ARRIVAL_AT_DESTINATION_BY_TRANSPORTER_DATE\"]\n",
    "#        - filtered_df[\"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\"]\n",
    "#    ).dt.days #\n",
    "#\n",
    "#    expected_lead_time_days = (\n",
    "#        filtered_df[\"DT_EXPECTED_DELIVERY_TO_FINAL_CUSTOMER_DATE\"]\n",
    "#        - filtered_df[\"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\"]\n",
    "#    ).dt.days #\n",
    "#\n",
    "#    filtered_df[\"TOTAL_LEAD_VS_EXPECTED_DAYS\"] = actual_lead_time_days - expected_lead_time_days #\n",
    "#    print(\"Created 'TOTAL_LEAD_VS_EXPECTED_DAYS' feature.\")\n",
    "# else:\n",
    "#    print(\"Warning: Required columns missing for 'TOTAL_LEAD_VS_EXPECTED_DAYS'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63f52bbd-3814-40a6-aee3-56997f237d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag for missing shipping order date (This was already created as 'MISSING_SHIPPING_ORDER_FLAG' during imputation)\n",
    "# Re-creating it as per original script logic (might be redundant if previous flags are kept)\n",
    "# if \"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\" in filtered_df.columns:\n",
    "#    filtered_df[\"MISSING_SHIPPING_ORDER_DATE_FE_FLAG\"] = (\n",
    "#        filtered_df[\"DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE\"].isna().astype(int)\n",
    "#    ) #\n",
    "# Note: If imputation was successful, this should be all zeros. If NaNs persist, it means imputation failed or wasn't applicable.\n",
    "#    print(\"Created/updated 'MISSING_SHIPPING_ORDER_DATE_FE_FLAG'.\")\n",
    "# else:\n",
    "#    print(\"Warning: 'DT_SHIPPING_ORDER_TO_NSC_COMPOUND_CREATION_DATE' not found for flag creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f56a64e1-6464-4436-b690-2a4057042091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag for missing customs inbound date\n",
    "# if \"DT_CUSTOMS_OFFICE_INBOUND_DATE\" in filtered_df.columns:\n",
    "#    filtered_df[\"MISSING_CUSTOMS_INBOUND_DATE_FE_FLAG\"] = (\n",
    "#        filtered_df[\"DT_CUSTOMS_OFFICE_INBOUND_DATE\"].isna().astype(int)\n",
    "#    ) #\n",
    "#    # This flag will be 1 for domestic shipments or where data was truly missing.\n",
    "#    print(\"Created 'MISSING_CUSTOMS_INBOUND_DATE_FE_FLAG'.\")\n",
    "# else:\n",
    "#    print(\"Warning: 'DT_CUSTOMS_OFFICE_INBOUND_DATE' not found for flag creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14fb9d8a-62c2-4645-89f4-0fb357a3813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'ORDER_DAY_OF_WEEK' feature.\n"
     ]
    }
   ],
   "source": [
    "# Day of the week for the commercial order date (Monday=0, Sunday=6)\n",
    "if \"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\" in filtered_df.columns:\n",
    "    filtered_df[\"ORDER_DAY_OF_WEEK\"] = filtered_df[\n",
    "        \"DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE\"\n",
    "    ].dt.dayofweek  #\n",
    "    print(\"Created 'ORDER_DAY_OF_WEEK' feature.\")\n",
    "else:\n",
    "    print(\n",
    "        \"Warning: 'DT_COMMERCIAL_ORDER_FIRST_ENTRY_DATE' not found for 'ORDER_DAY_OF_WEEK'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02632a15-3839-42de-b954-995717c58293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month of the vehicle factory production date\n",
    "# if \"DT_VEHICLE_FACTORY_PRODUCTION_DATE\" in filtered_df.columns:\n",
    "# Ensure the column is datetime before accessing .dt accessor\n",
    "#    if pd.api.types.is_datetime64_any_dtype(filtered_df[\"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"]):\n",
    "#        filtered_df[\"PRODUCTION_MONTH\"] = (\n",
    "#            filtered_df[\"DT_VEHICLE_FACTORY_PRODUCTION_DATE\"].dt.month\n",
    "#        ) #\n",
    "#        print(\"Created 'PRODUCTION_MONTH' feature.\")\n",
    "#    else:\n",
    "#        print(\"Warning: 'DT_VEHICLE_FACTORY_PRODUCTION_DATE' is not datetime. Cannot create 'PRODUCTION_MONTH'.\")\n",
    "# else:\n",
    "#    print(\"Warning: 'DT_VEHICLE_FACTORY_PRODUCTION_DATE' not found for 'PRODUCTION_MONTH'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_9_intro",
   "metadata": {},
   "source": [
    "## 9. Final Data Cleaning and Validation\n",
    "This section performs a final check for missing values and applies specific handling for remaining NaNs in certain columns based on the original script's logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bdd476d-3444-42b0-9bbe-23bb645e0fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with remaining missing values before final handling:\n",
      "- DT_CUSTOMS_OFFICE_INBOUND_DATE: 7109 missing values\n",
      "- DT_CUSTOMS_OFFICE_OUTBOUND_DATE: 7108 missing values\n",
      "- CD_DESTINATION_LOGISTIC_COMPOUND_CODE: 5189 missing values\n",
      "- CD_DESTINATION_LOGISTIC_COMPOUND_TYPE_CODE: 5189 missing values\n",
      "- CD_SOURCE_SYSTEM_VEHICLE_DEMONSTRATION_STATUS_CODE: 4654 missing values\n",
      "- GN_CUSTOMER_SALES_CHANNEL_NAME: 706 missing values\n",
      "- CD_DISTRIBUTION_SALES_SUBCHANNEL_ORIGINAL_CODE: 4922 missing values\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with any remaining missing values\n",
    "remaining_na_summary = filtered_df.isna().sum()  #\n",
    "columns_with_remaining_na = remaining_na_summary[remaining_na_summary > 0].to_dict()  #\n",
    "\n",
    "if columns_with_remaining_na:\n",
    "    print(\"Columns with remaining missing values before final handling:\")\n",
    "    for col, count in columns_with_remaining_na.items():\n",
    "        print(f\"- {col}: {count} missing values\")\n",
    "else:\n",
    "    print(\n",
    "        \"No missing values remaining in the DataFrame before final specific handling.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ddd423b0-0038-4ebd-8ecf-80fed03eaf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped rare codes in 'CD_DESTINATION_LOGISTIC_COMPOUND_CODE' into 'Other'.\n",
      "Filled NaNs in 'CD_DESTINATION_LOGISTIC_COMPOUND_CODE' with 'Unknown'.\n"
     ]
    }
   ],
   "source": [
    "# Handle 'CD_DESTINATION_LOGISTIC_COMPOUND_CODE'\n",
    "compound_code_col = \"CD_DESTINATION_LOGISTIC_COMPOUND_CODE\"\n",
    "if compound_code_col in filtered_df.columns:\n",
    "    # Group rare codes (less than 1% frequency) into \"Other\"\n",
    "    if (\n",
    "        filtered_df[compound_code_col].notna().any()\n",
    "    ):  # Ensure there are non-NA values to calculate frequencies\n",
    "        compound_code_counts = filtered_df[compound_code_col].value_counts(\n",
    "            normalize=True\n",
    "        )  #\n",
    "        rare_codes = compound_code_counts[compound_code_counts < 0.01].index.tolist()  #\n",
    "        if rare_codes:\n",
    "            filtered_df[compound_code_col] = filtered_df[compound_code_col].replace(\n",
    "                rare_codes, \"Other\"\n",
    "            )  #\n",
    "            print(f\"Grouped rare codes in '{compound_code_col}' into 'Other'.\")\n",
    "\n",
    "    # Impute remaining missing values (NaNs) as \"Unknown\"\n",
    "    if filtered_df[compound_code_col].isna().any():\n",
    "        filtered_df[compound_code_col].fillna(\"Unknown\", inplace=True)  #\n",
    "        print(f\"Filled NaNs in '{compound_code_col}' with 'Unknown'.\")\n",
    "else:\n",
    "    print(f\"Column '{compound_code_col}' not found for final handling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d909acf4-62fe-4844-b90a-55b3cacef762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled NaNs in 'CD_DESTINATION_LOGISTIC_COMPOUND_TYPE_CODE' with 'Unknown'.\n",
      "Created 'Destination_Type' feature by combining compound code and type.\n"
     ]
    }
   ],
   "source": [
    "# Handle 'CD_DESTINATION_LOGISTIC_COMPOUND_TYPE_CODE' and create 'Destination_Type'\n",
    "compound_type_col = \"CD_DESTINATION_LOGISTIC_COMPOUND_TYPE_CODE\"\n",
    "if compound_type_col in filtered_df.columns:\n",
    "    # Impute missing type codes with \"Unknown\"\n",
    "    if filtered_df[compound_type_col].isna().any():\n",
    "        filtered_df[compound_type_col].fillna(\"Unknown\", inplace=True)  #\n",
    "        print(f\"Filled NaNs in '{compound_type_col}' with 'Unknown'.\")\n",
    "\n",
    "    # Combine compound code and type into a new feature 'Destination_Type'\n",
    "    if compound_code_col in filtered_df.columns:  # Ensure the code column also exists\n",
    "        filtered_df[\"Destination_Type\"] = (\n",
    "            filtered_df[compound_code_col].astype(str)\n",
    "            + \"_\"\n",
    "            + filtered_df[compound_type_col].astype(str)\n",
    "        )  #\n",
    "        print(\"Created 'Destination_Type' feature by combining compound code and type.\")\n",
    "    else:\n",
    "        print(f\"Cannot create 'Destination_Type' as '{compound_code_col}' is missing.\")\n",
    "else:\n",
    "    print(f\"Column '{compound_type_col}' not found for final handling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0405ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'IS_DEMONSTRATION_VEHICLE' flag.\n",
      "Dropped original column 'CD_SOURCE_SYSTEM_VEHICLE_DEMONSTRATION_STATUS_CODE'.\n"
     ]
    }
   ],
   "source": [
    "# Handle 'CD_SOURCE_SYSTEM_VEHICLE_DEMONSTRATION_STATUS_CODE' by creating 'IS_DEMONSTRATION_VEHICLE' flag\n",
    "demo_status_code_col = \"CD_SOURCE_SYSTEM_VEHICLE_DEMONSTRATION_STATUS_CODE\"\n",
    "if demo_status_code_col in filtered_df.columns:\n",
    "    # If code exists (not NaN), it's a demonstration vehicle (1), otherwise not (0).\n",
    "    filtered_df[\"IS_DEMONSTRATION_VEHICLE\"] = (\n",
    "        filtered_df[demo_status_code_col].notna().astype(int)\n",
    "    )  #\n",
    "    # The original script also fills NaNs in the new flag with 0, which notna().astype(int) already does implicitly.\n",
    "    # filtered_df[\"IS_DEMONSTRATION_VEHICLE\"].fillna(0, inplace=True) # Redundant but harmless\n",
    "    print(\"Created 'IS_DEMONSTRATION_VEHICLE' flag.\")\n",
    "\n",
    "    # Drop the original demonstration status code column\n",
    "    filtered_df.drop(columns=[demo_status_code_col], inplace=True)  #\n",
    "    print(f\"Dropped original column '{demo_status_code_col}'.\")\n",
    "else:\n",
    "    print(\n",
    "        f\"Column '{demo_status_code_col}' not found for creating demonstration vehicle flag.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9956eaed-23c9-45db-a142-686a088a0c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remplacé les NaN dans 'CD_DISTRIBUTION_SALES_SUBCHANNEL_ORIGINAL_CODE' par 'Unknown'.\n"
     ]
    }
   ],
   "source": [
    "dist_subchannel_col = \"CD_DISTRIBUTION_SALES_SUBCHANNEL_ORIGINAL_CODE\"\n",
    "\n",
    "if dist_subchannel_col in filtered_df.columns:\n",
    "    if filtered_df[dist_subchannel_col].isna().any():\n",
    "        # Convertir la colonne en type chaîne de caractères avant de remplir les NaN\n",
    "        filtered_df[dist_subchannel_col] = filtered_df[dist_subchannel_col].astype(str)\n",
    "        # Remplacer les NaN par 'Unknown'\n",
    "        filtered_df[dist_subchannel_col] = filtered_df[dist_subchannel_col].fillna(\n",
    "            \"Unknown\"\n",
    "        )\n",
    "        print(f\"Remplacé les NaN dans '{dist_subchannel_col}' par 'Unknown'.\")\n",
    "    else:\n",
    "        # S'assurer que la colonne est de type chaîne de caractères\n",
    "        filtered_df[dist_subchannel_col] = filtered_df[dist_subchannel_col].astype(str)\n",
    "        print(\n",
    "            f\"Assuré que '{dist_subchannel_col}' est de type chaîne de caractères pour l'export Parquet.\"\n",
    "        )\n",
    "else:\n",
    "    print(f\"Colonne '{dist_subchannel_col}' non trouvée pour le traitement final.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e43be92b-d10b-49c0-9718-cd1dede9c4ae_final_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL REPORT: Columns with remaining missing values ---\n",
      "- DT_CUSTOMS_OFFICE_INBOUND_DATE: 7109 missing values\n",
      "- DT_CUSTOMS_OFFICE_OUTBOUND_DATE: 7108 missing values\n",
      "- GN_CUSTOMER_SALES_CHANNEL_NAME: 706 missing values\n",
      "These may be expected (e.g., customs dates for domestic) or require further investigation.\n",
      "\n",
      "Final shape of the processed DataFrame: (9470, 111)\n"
     ]
    }
   ],
   "source": [
    "# Final check for any remaining missing values after all processing\n",
    "final_na_summary = filtered_df.isna().sum()\n",
    "final_columns_with_na = final_na_summary[final_na_summary > 0].to_dict()  #\n",
    "\n",
    "if final_columns_with_na:\n",
    "    print(\"\\n--- FINAL REPORT: Columns with remaining missing values ---\")\n",
    "    for col, count in final_columns_with_na.items():\n",
    "        print(f\"- {col}: {count} missing values\")\n",
    "    print(\n",
    "        \"These may be expected (e.g., customs dates for domestic) or require further investigation.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"\\n--- FINAL REPORT: No missing values remaining in the DataFrame. All handled! ---\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFinal shape of the processed DataFrame: {filtered_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_10_intro",
   "metadata": {},
   "source": [
    "## 10. Saving Processed Data (Optional)\n",
    "This section is a placeholder for saving the fully processed DataFrame to a new file (e.g., CSV or Parquet) for use in modeling or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "save_data_placeholder",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed DataFrame successfully saved to: ./data/processed_data.csv\n",
      "\n",
      "Processed DataFrame successfully saved to: ./data/processed_data.parquet\n",
      "\n",
      "Data processing and feature engineering complete. Uncomment saving code if needed.\n"
     ]
    }
   ],
   "source": [
    "# Example: Save the processed DataFrame to a CSV file\n",
    "output_file_path = \"./data/processed_data.csv\"\n",
    "try:\n",
    "    filtered_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"\\nProcessed DataFrame successfully saved to: {output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving DataFrame: {e}\")\n",
    "\n",
    "# Example: Save to Parquet for better performance and type preservation\n",
    "output_parquet_path = \"./data/processed_data.parquet\"\n",
    "try:\n",
    "    filtered_df.to_parquet(output_parquet_path, index=False)\n",
    "    print(f\"\\nProcessed DataFrame successfully saved to: {output_parquet_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving DataFrame to Parquet: {e}\")\n",
    "\n",
    "print(\n",
    "    \"\\nData processing and feature engineering complete. Uncomment saving code if needed.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
